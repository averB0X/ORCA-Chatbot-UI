{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intents dictionary\n",
    "dictionary = open('../intents/intents.json').read()\n",
    "intents = json.loads(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', ',', '!', '?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ignored characters\n",
    "# words -> list of tokenized words\n",
    "# tags -> tags defined from dictionary\n",
    "# documents -> word (tokenized) and tag relation\n",
    "exclude = ['.', ',', '!', '?']\n",
    "words = []  # list for each tokenized words (words are separated from each other in a statement/phrase)\n",
    "classes = []  # class or label (tags)\n",
    "documents = []  # list for the combinations, where each tokenized words belong in relation to tags\n",
    "\n",
    "exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: \n",
      "hi\n",
      "hello\n",
      "hey\n",
      "good\n",
      "morning\n",
      "good\n",
      "evening\n",
      "greetings\n",
      "what\n",
      "'s\n",
      "up\n",
      "?\n",
      "how\n",
      "are\n",
      "you\n",
      "doing\n",
      "?\n",
      "good\n",
      "day\n",
      "what\n",
      "are\n",
      "you\n",
      "called\n",
      "?\n",
      "who\n",
      "are\n",
      "you\n",
      "?\n",
      "what\n",
      "is\n",
      "your\n",
      "name\n",
      "?\n",
      "requirements\n",
      "form\n",
      "138\n",
      "form\n",
      "137\n",
      "report\n",
      "card\n",
      "grades\n",
      "marks\n",
      "subjects\n",
      "courses\n",
      "documents\n",
      "remaining\n",
      "requirements\n",
      "course\n",
      "schedule\n",
      "subject\n",
      "schedule\n",
      "course\n",
      "schedule\n",
      "what\n",
      "is\n",
      "my\n",
      "schedule\n",
      "for\n",
      "this\n",
      "subject\n",
      "?\n",
      "what\n",
      "is\n",
      "my\n",
      "schedule\n",
      "for\n",
      "this\n",
      "course\n",
      "?\n",
      "what\n",
      "time\n",
      "does\n",
      "my\n",
      "class\n",
      "start\n",
      "?\n",
      "how\n",
      "can\n",
      "I\n",
      "change\n",
      "my\n",
      "schedule\n",
      "?\n",
      "i\n",
      "need\n",
      "to\n",
      "change\n",
      "my\n",
      "schedule\n",
      "how\n",
      "can\n",
      "I\n",
      "enroll\n",
      "on\n",
      "a\n",
      "subject\n",
      "?\n",
      "how\n",
      "can\n",
      "I\n",
      "drop\n",
      "a\n",
      "subject\n",
      "?\n",
      "i\n",
      "need\n",
      "to\n",
      "pass\n",
      "requirements\n",
      "where\n",
      "do\n",
      "I\n",
      "pass\n",
      "requirements\n",
      "?\n",
      "what\n",
      "are\n",
      "my\n",
      "missing\n",
      "documents\n",
      "?\n",
      "payment\n",
      "pay\n",
      "bill\n",
      "alternative\n",
      "payment\n",
      "gcash\n",
      "cash\n",
      "bank\n",
      "online\n",
      "payment\n",
      "balance\n",
      "installment\n",
      "how\n",
      "much\n",
      "is\n",
      "my\n",
      "remaining\n",
      "balance\n",
      "?\n",
      "how\n",
      "do\n",
      "I\n",
      "pay\n",
      "?\n",
      "how\n",
      "can\n",
      "I\n",
      "pay\n",
      "?\n",
      "where\n",
      "can\n",
      "I\n",
      "pay\n",
      "?\n",
      "what\n",
      "are\n",
      "the\n",
      "ways\n",
      "I\n",
      "can\n",
      "pay\n",
      "?\n",
      "can\n",
      "I\n",
      "pay\n",
      "online\n",
      "?\n",
      "can\n",
      "I\n",
      "pay\n",
      "onsite\n",
      "?\n",
      "where\n",
      "is\n",
      "the\n",
      "cashier\n",
      "?\n",
      "what\n",
      "are\n",
      "the\n",
      "office\n",
      "hours\n",
      "of\n",
      "cashier\n",
      "?\n",
      "can\n",
      "I\n",
      "pay\n",
      "another\n",
      "way\n",
      "?\n",
      "how\n",
      "much\n",
      "do\n",
      "I\n",
      "need\n",
      "to\n",
      "pay\n",
      "?\n",
      "do\n",
      "I\n",
      "need\n",
      "to\n",
      "pay\n",
      "anything\n",
      "this\n",
      "month\n",
      "?\n",
      "do\n",
      "I\n",
      "have\n",
      "remaining\n",
      "balance\n",
      "?\n",
      "my\n",
      "payment\n",
      "did\n",
      "n't\n",
      "reflect\n",
      "my\n",
      "payment\n",
      "did\n",
      "not\n",
      "go\n",
      "through\n",
      "account\n",
      "ID\n",
      "RFID\n",
      "microsoft\n",
      "account\n",
      "elms\n",
      "account\n",
      "one\n",
      "sti\n",
      "account\n",
      "one\n",
      "sti\n",
      "portal\n",
      "i\n",
      "cant\n",
      "tap\n",
      "my\n",
      "ID\n",
      "my\n",
      "ID\n",
      "does\n",
      "n't\n",
      "work\n",
      "my\n",
      "ID\n",
      "doens't\n",
      "work\n",
      "in\n",
      "the\n",
      "gate\n",
      "my\n",
      "ID\n",
      "is\n",
      "broken\n",
      "my\n",
      "RFID\n",
      "does\n",
      "n't\n",
      "work\n",
      "i\n",
      "need\n",
      "an\n",
      "ID\n",
      "replacement\n",
      "i\n",
      "'m\n",
      "having\n",
      "problems\n",
      "with\n",
      "my\n",
      "ID\n",
      "i\n",
      "ca\n",
      "n't\n",
      "access\n",
      "my\n",
      "account\n",
      "i\n",
      "ca\n",
      "n't\n",
      "open\n",
      "my\n",
      "account\n",
      "i\n",
      "ca\n",
      "n't\n",
      "open\n",
      "my\n",
      "microsoft\n",
      "account\n",
      "i\n",
      "ca\n",
      "n't\n",
      "login\n",
      "i\n",
      "cant\n",
      "log\n",
      "my\n",
      "account\n",
      "in\n",
      "my\n",
      "account\n",
      "does\n",
      "n't\n",
      "work\n",
      "i\n",
      "cant\n",
      "open\n",
      "my\n",
      "eLMS\n",
      "i\n",
      "cant\n",
      "open\n",
      "my\n",
      "portal\n",
      "how\n",
      "do\n",
      "i\n",
      "access\n",
      "my\n",
      "account\n",
      "?\n",
      "how\n",
      "can\n",
      "i\n",
      "reset\n",
      "my\n",
      "password\n",
      "?\n",
      "i\n",
      "need\n",
      "help\n",
      "with\n",
      "my\n",
      "account\n",
      "how\n",
      "do\n",
      "i\n",
      "open\n",
      "my\n",
      "portal\n",
      "?\n",
      "can\n",
      "you\n",
      "help\n",
      "me\n",
      "reset\n",
      "my\n",
      "password\n",
      "?\n",
      "can\n",
      "you\n",
      "help\n",
      "me\n",
      "access\n",
      "my\n",
      "account\n",
      "?\n",
      "im\n",
      "getting\n",
      "an\n",
      "error\n",
      "website\n",
      "is\n",
      "displaying\n",
      "an\n",
      "error\n",
      "portal\n",
      "has\n",
      "server\n",
      "error\n",
      "what\n",
      "is\n",
      "the\n",
      "status\n",
      "of\n",
      "the\n",
      "portal\n",
      "?\n",
      "i\n",
      "ca\n",
      "n't\n",
      "access\n",
      "my\n",
      "portal\n",
      "programs\n",
      "what\n",
      "programs\n",
      "do\n",
      "you\n",
      "offer\n",
      "?\n",
      "courses\n",
      "what\n",
      "courses\n",
      "do\n",
      "you\n",
      "offer\n",
      "?\n",
      "what\n",
      "programs\n",
      "does\n",
      "STI\n",
      "have\n",
      "?\n",
      "do\n",
      "you\n",
      "have\n",
      "BSIT\n",
      "do\n",
      "you\n",
      "have\n",
      "BSCS\n",
      "?\n",
      "do\n",
      "you\n",
      "offer\n",
      "computer\n",
      "science\n",
      "information\n",
      "technology\n",
      "BSTM\n",
      "hospitality\n",
      "and\n",
      "management\n",
      "Classes: \n",
      "greetings\n",
      "name\n",
      "context: registrar\n",
      "context: cashier\n",
      "context: technical\n",
      "programs\n",
      "[(['hi'], 'greetings'), (['hello'], 'greetings'), (['hey'], 'greetings'), (['good', 'morning'], 'greetings'), (['good', 'evening'], 'greetings'), (['greetings'], 'greetings'), (['what', \"'s\", 'up', '?'], 'greetings'), (['how', 'are', 'you', 'doing', '?'], 'greetings'), (['good', 'day'], 'greetings'), (['what', 'are', 'you', 'called', '?'], 'name'), (['who', 'are', 'you', '?'], 'name'), (['what', 'is', 'your', 'name', '?'], 'name'), (['requirements'], 'context: registrar'), (['form', '138'], 'context: registrar'), (['form', '137'], 'context: registrar'), (['report', 'card'], 'context: registrar'), (['grades'], 'context: registrar'), (['marks'], 'context: registrar'), (['subjects'], 'context: registrar'), (['courses'], 'context: registrar'), (['documents'], 'context: registrar'), (['remaining', 'requirements'], 'context: registrar'), (['course', 'schedule'], 'context: registrar'), (['subject', 'schedule'], 'context: registrar'), (['course', 'schedule'], 'context: registrar'), (['what', 'is', 'my', 'schedule', 'for', 'this', 'subject', '?'], 'context: registrar'), (['what', 'is', 'my', 'schedule', 'for', 'this', 'course', '?'], 'context: registrar'), (['what', 'time', 'does', 'my', 'class', 'start', '?'], 'context: registrar'), (['how', 'can', 'I', 'change', 'my', 'schedule', '?'], 'context: registrar'), (['i', 'need', 'to', 'change', 'my', 'schedule'], 'context: registrar'), (['how', 'can', 'I', 'enroll', 'on', 'a', 'subject', '?'], 'context: registrar'), (['how', 'can', 'I', 'drop', 'a', 'subject', '?'], 'context: registrar'), (['i', 'need', 'to', 'pass', 'requirements'], 'context: registrar'), (['where', 'do', 'I', 'pass', 'requirements', '?'], 'context: registrar'), (['what', 'are', 'my', 'missing', 'documents', '?'], 'context: registrar'), (['payment'], 'context: cashier'), (['pay'], 'context: cashier'), (['bill'], 'context: cashier'), (['alternative', 'payment'], 'context: cashier'), (['gcash'], 'context: cashier'), (['cash'], 'context: cashier'), (['bank'], 'context: cashier'), (['online', 'payment'], 'context: cashier'), (['balance'], 'context: cashier'), (['installment'], 'context: cashier'), (['how', 'much', 'is', 'my', 'remaining', 'balance', '?'], 'context: cashier'), (['how', 'do', 'I', 'pay', '?'], 'context: cashier'), (['how', 'can', 'I', 'pay', '?'], 'context: cashier'), (['where', 'can', 'I', 'pay', '?'], 'context: cashier'), (['what', 'are', 'the', 'ways', 'I', 'can', 'pay', '?'], 'context: cashier'), (['can', 'I', 'pay', 'online', '?'], 'context: cashier'), (['can', 'I', 'pay', 'onsite', '?'], 'context: cashier'), (['where', 'is', 'the', 'cashier', '?'], 'context: cashier'), (['what', 'are', 'the', 'office', 'hours', 'of', 'cashier', '?'], 'context: cashier'), (['can', 'I', 'pay', 'another', 'way', '?'], 'context: cashier'), (['how', 'much', 'do', 'I', 'need', 'to', 'pay', '?'], 'context: cashier'), (['do', 'I', 'need', 'to', 'pay', 'anything', 'this', 'month', '?'], 'context: cashier'), (['do', 'I', 'have', 'remaining', 'balance', '?'], 'context: cashier'), (['my', 'payment', 'did', \"n't\", 'reflect'], 'context: cashier'), (['my', 'payment', 'did', 'not', 'go', 'through'], 'context: cashier'), (['account'], 'context: technical'), (['ID'], 'context: technical'), (['RFID'], 'context: technical'), (['microsoft', 'account'], 'context: technical'), (['elms', 'account'], 'context: technical'), (['one', 'sti', 'account'], 'context: technical'), (['one', 'sti'], 'context: technical'), (['portal'], 'context: technical'), (['i', 'cant', 'tap', 'my', 'ID'], 'context: technical'), (['my', 'ID', 'does', \"n't\", 'work'], 'context: technical'), (['my', 'ID', \"doens't\", 'work', 'in', 'the', 'gate'], 'context: technical'), (['my', 'ID', 'is', 'broken'], 'context: technical'), (['my', 'RFID', 'does', \"n't\", 'work'], 'context: technical'), (['i', 'need', 'an', 'ID', 'replacement'], 'context: technical'), (['i', \"'m\", 'having', 'problems', 'with', 'my', 'ID'], 'context: technical'), (['i', 'ca', \"n't\", 'access', 'my', 'account'], 'context: technical'), (['i', 'ca', \"n't\", 'open', 'my', 'account'], 'context: technical'), (['i', 'ca', \"n't\", 'open', 'my', 'microsoft', 'account'], 'context: technical'), (['i', 'ca', \"n't\", 'login'], 'context: technical'), (['i', 'cant', 'log', 'my', 'account', 'in'], 'context: technical'), (['my', 'account', 'does', \"n't\", 'work'], 'context: technical'), (['i', 'cant', 'open', 'my', 'eLMS'], 'context: technical'), (['i', 'cant', 'open', 'my', 'portal'], 'context: technical'), (['how', 'do', 'i', 'access', 'my', 'account', '?'], 'context: technical'), (['how', 'can', 'i', 'reset', 'my', 'password', '?'], 'context: technical'), (['i', 'need', 'help', 'with', 'my', 'account'], 'context: technical'), (['how', 'do', 'i', 'open', 'my', 'portal', '?'], 'context: technical'), (['can', 'you', 'help', 'me', 'reset', 'my', 'password', '?'], 'context: technical'), (['can', 'you', 'help', 'me', 'access', 'my', 'account', '?'], 'context: technical'), (['im', 'getting', 'an', 'error'], 'context: technical'), (['website', 'is', 'displaying', 'an', 'error'], 'context: technical'), (['portal', 'has', 'server', 'error'], 'context: technical'), (['what', 'is', 'the', 'status', 'of', 'the', 'portal', '?'], 'context: technical'), (['i', 'ca', \"n't\", 'access', 'my', 'portal'], 'context: technical'), (['programs'], 'programs'), (['what', 'programs', 'do', 'you', 'offer', '?'], 'programs'), (['courses'], 'programs'), (['what', 'courses', 'do', 'you', 'offer', '?'], 'programs'), (['what', 'programs', 'does', 'STI', 'have', '?'], 'programs'), (['do', 'you', 'have', 'BSIT'], 'programs'), (['do', 'you', 'have', 'BSCS', '?'], 'programs'), (['do', 'you', 'offer', 'computer', 'science'], 'programs'), (['information', 'technology'], 'programs'), (['BSTM'], 'programs'), (['hospitality', 'and', 'management'], 'programs')]\n"
     ]
    }
   ],
   "source": [
    "# iterates over the dictionary\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        tokens = nltk.word_tokenize(pattern)\n",
    "        words.extend(tokens)\n",
    "        documents.append((tokens, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# output         \n",
    "print(\"Words: \")\n",
    "for i in words:\n",
    "    print(i)\n",
    "\n",
    "print(\"Classes: \")\n",
    "for i in classes:\n",
    "    print(i)\n",
    "    \n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'hello', 'hey', 'good', 'morning', 'good', 'evening', 'greeting', 'what', \"'s\", 'up', 'how', 'are', 'you', 'doing', 'good', 'day', 'what', 'are', 'you', 'called', 'who', 'are', 'you', 'what', 'is', 'your', 'name', 'requirement', 'form', '138', 'form', '137', 'report', 'card', 'grade', 'mark', 'subject', 'course', 'document', 'remaining', 'requirement', 'course', 'schedule', 'subject', 'schedule', 'course', 'schedule', 'what', 'is', 'my', 'schedule', 'for', 'this', 'subject', 'what', 'is', 'my', 'schedule', 'for', 'this', 'course', 'what', 'time', 'doe', 'my', 'class', 'start', 'how', 'can', 'I', 'change', 'my', 'schedule', 'i', 'need', 'to', 'change', 'my', 'schedule', 'how', 'can', 'I', 'enroll', 'on', 'a', 'subject', 'how', 'can', 'I', 'drop', 'a', 'subject', 'i', 'need', 'to', 'pas', 'requirement', 'where', 'do', 'I', 'pas', 'requirement', 'what', 'are', 'my', 'missing', 'document', 'payment', 'pay', 'bill', 'alternative', 'payment', 'gcash', 'cash', 'bank', 'online', 'payment', 'balance', 'installment', 'how', 'much', 'is', 'my', 'remaining', 'balance', 'how', 'do', 'I', 'pay', 'how', 'can', 'I', 'pay', 'where', 'can', 'I', 'pay', 'what', 'are', 'the', 'way', 'I', 'can', 'pay', 'can', 'I', 'pay', 'online', 'can', 'I', 'pay', 'onsite', 'where', 'is', 'the', 'cashier', 'what', 'are', 'the', 'office', 'hour', 'of', 'cashier', 'can', 'I', 'pay', 'another', 'way', 'how', 'much', 'do', 'I', 'need', 'to', 'pay', 'do', 'I', 'need', 'to', 'pay', 'anything', 'this', 'month', 'do', 'I', 'have', 'remaining', 'balance', 'my', 'payment', 'did', \"n't\", 'reflect', 'my', 'payment', 'did', 'not', 'go', 'through', 'account', 'ID', 'RFID', 'microsoft', 'account', 'elm', 'account', 'one', 'sti', 'account', 'one', 'sti', 'portal', 'i', 'cant', 'tap', 'my', 'ID', 'my', 'ID', 'doe', \"n't\", 'work', 'my', 'ID', \"doens't\", 'work', 'in', 'the', 'gate', 'my', 'ID', 'is', 'broken', 'my', 'RFID', 'doe', \"n't\", 'work', 'i', 'need', 'an', 'ID', 'replacement', 'i', \"'m\", 'having', 'problem', 'with', 'my', 'ID', 'i', 'ca', \"n't\", 'access', 'my', 'account', 'i', 'ca', \"n't\", 'open', 'my', 'account', 'i', 'ca', \"n't\", 'open', 'my', 'microsoft', 'account', 'i', 'ca', \"n't\", 'login', 'i', 'cant', 'log', 'my', 'account', 'in', 'my', 'account', 'doe', \"n't\", 'work', 'i', 'cant', 'open', 'my', 'eLMS', 'i', 'cant', 'open', 'my', 'portal', 'how', 'do', 'i', 'access', 'my', 'account', 'how', 'can', 'i', 'reset', 'my', 'password', 'i', 'need', 'help', 'with', 'my', 'account', 'how', 'do', 'i', 'open', 'my', 'portal', 'can', 'you', 'help', 'me', 'reset', 'my', 'password', 'can', 'you', 'help', 'me', 'access', 'my', 'account', 'im', 'getting', 'an', 'error', 'website', 'is', 'displaying', 'an', 'error', 'portal', 'ha', 'server', 'error', 'what', 'is', 'the', 'status', 'of', 'the', 'portal', 'i', 'ca', \"n't\", 'access', 'my', 'portal', 'program', 'what', 'program', 'do', 'you', 'offer', 'course', 'what', 'course', 'do', 'you', 'offer', 'what', 'program', 'doe', 'STI', 'have', 'do', 'you', 'have', 'BSIT', 'do', 'you', 'have', 'BSCS', 'do', 'you', 'offer', 'computer', 'science', 'information', 'technology', 'BSTM', 'hospitality', 'and', 'management']\n"
     ]
    }
   ],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "words = [lem.lemmatize(word) for word in words if word not in exclude]  # if word is not in exclude, lemmatize word\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'m\", \"'s\", '137', '138', 'BSCS', 'BSIT', 'BSTM', 'I', 'ID', 'RFID', 'STI', 'a', 'access', 'account', 'alternative', 'an', 'and', 'another', 'anything', 'are', 'balance', 'bank', 'bill', 'broken', 'ca', 'called', 'can', 'cant', 'card', 'cash', 'cashier', 'change', 'class', 'computer', 'course', 'day', 'did', 'displaying', 'do', 'document', 'doe', \"doens't\", 'doing', 'drop', 'eLMS', 'elm', 'enroll', 'error', 'evening', 'for', 'form', 'gate', 'gcash', 'getting', 'go', 'good', 'grade', 'greeting', 'ha', 'have', 'having', 'hello', 'help', 'hey', 'hi', 'hospitality', 'hour', 'how', 'i', 'im', 'in', 'information', 'installment', 'is', 'log', 'login', 'management', 'mark', 'me', 'microsoft', 'missing', 'month', 'morning', 'much', 'my', \"n't\", 'name', 'need', 'not', 'of', 'offer', 'office', 'on', 'one', 'online', 'onsite', 'open', 'pas', 'password', 'pay', 'payment', 'portal', 'problem', 'program', 'reflect', 'remaining', 'replacement', 'report', 'requirement', 'reset', 'schedule', 'science', 'server', 'start', 'status', 'sti', 'subject', 'tap', 'technology', 'the', 'this', 'through', 'time', 'to', 'up', 'way', 'website', 'what', 'where', 'who', 'with', 'work', 'you', 'your']\n",
      "['context: cashier', 'context: registrar', 'context: technical', 'greetings', 'name', 'programs']\n"
     ]
    }
   ],
   "source": [
    "words = sorted(set(words))  # removes duplicated words\n",
    "classes = sorted(set(classes))  # removes duplicate tags\n",
    "\n",
    "#output\n",
    "print(words)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializes each element | wb -> writing binary | outputs a pickle file (.pkl)\n",
    "pickle.dump(words, open('../pkl/words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('../pkl/tags.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<============= MACHINE LEARNING =============>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "outputEmpty = [0] * len(classes)  # template of zeroes (0), however many classes there are\n",
    "\n",
    "# output\n",
    "print(outputEmpty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in documents:\n",
    "    bag = []  # for each combination(documents), creates an empty bag of words\n",
    "    wordPatterns = document[0]\n",
    "    wordPatterns = [lem.lemmatize(word.lower()) for word in wordPatterns]  # lemmatize each word in wordPatters, which consist of the index 0 (words) in each document element\n",
    "    \n",
    "    # inputs 1 or 0 into the bag of words depending whether it occurs in the pattern or not, respectively\n",
    "    for word in words:\n",
    "        if word in wordPatterns:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['hospitality', 'and', 'management'], 'programs')\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "outputRow = list(outputEmpty)\n",
    "outputRow[classes.index(document[1])] = 1\n",
    "training.append(bag + outputRow)  # stores the value of bag (document[0]) and outputRow (document[1]) to training list which is either 1 or 0\n",
    "\n",
    "# output\n",
    "print(document)\n",
    "print(bag)\n",
    "print(outputRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training)  # shuffles training data\n",
    "training = np.array(training)  # converts to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits the array into two dimensions, x for words and y for classes\n",
    "trainX = training[:, :len(words)]\n",
    "trainY = training[:, len(words):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aver\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# building the neural network\n",
    "model = Sequential([\n",
    "    # input layer with 128 neurons\n",
    "    # input shape is dependent to the shape of the training data for x\n",
    "    # activation function = rectified linear unit | if feature is determined to be significant label as 1, otherwise 0\n",
    "    Dense(128, input_shape=(len(trainX[0]),), activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    # output layer\n",
    "    # activation function = softmax | returns the probability that a certain input belongs to a specific class (tag)\n",
    "    Dense(len(trainY[0]), activation='softmax')  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Epoch 1/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 951ms/step - accuracy: 0.0000e+00 - loss: 1.6443\n",
      "Epoch 2/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.0000e+00 - loss: 1.8287\n",
      "Epoch 3/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.6008\n",
      "Epoch 4/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.2524\n",
      "Epoch 5/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.0652\n",
      "Epoch 6/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.3123\n",
      "Epoch 7/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.1482\n",
      "Epoch 8/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.7491\n",
      "Epoch 9/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.5359\n",
      "Epoch 10/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.3899\n",
      "Epoch 11/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.4075\n",
      "Epoch 12/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.8063\n",
      "Epoch 13/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.0433\n",
      "Epoch 14/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0993\n",
      "Epoch 15/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0651\n",
      "Epoch 16/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0049\n",
      "Epoch 17/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0030\n",
      "Epoch 18/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.1622\n",
      "Epoch 19/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0439\n",
      "Epoch 20/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 6.1391e-05\n",
      "Epoch 21/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 4.0773e-04\n",
      "Epoch 22/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0364\n",
      "Epoch 23/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0053\n",
      "Epoch 24/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0044\n",
      "Epoch 25/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 3.1156e-04\n",
      "Epoch 26/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0083\n",
      "Epoch 27/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.7325e-05\n",
      "Epoch 28/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.9012e-04\n",
      "Epoch 29/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0038\n",
      "Epoch 30/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 31/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0190\n",
      "Epoch 32/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 33/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0073\n",
      "Epoch 34/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.4543e-05\n",
      "Epoch 35/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.1491e-04\n",
      "Epoch 36/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 3.2186e-06\n",
      "Epoch 37/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 38/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 39/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 5.2748e-04\n",
      "Epoch 40/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 2.9444e-05\n",
      "Epoch 41/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 2.7748e-04\n",
      "Epoch 42/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 7.5991e-04\n",
      "Epoch 43/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.5687e-04\n",
      "Epoch 44/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 8.6199e-04\n",
      "Epoch 45/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.9989e-04\n",
      "Epoch 46/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 6.1152e-05\n",
      "Epoch 47/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 48/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 2.7179e-05\n",
      "Epoch 49/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 9.5367e-07\n",
      "Epoch 50/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 1.0000 - loss: 4.7206e-05\n",
      "Epoch 51/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 7.0178e-04\n",
      "Epoch 52/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.3792e-04\n",
      "Epoch 53/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 2.0265e-05\n",
      "Epoch 54/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.7534e-04\n",
      "Epoch 55/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0117\n",
      "Epoch 56/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 1.1086e-04\n",
      "Epoch 57/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.0169\n",
      "Epoch 58/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 4.7851e-04\n",
      "Epoch 59/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 9.9654e-05\n",
      "Epoch 60/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0091\n",
      "Epoch 61/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 4.5299e-06\n",
      "Epoch 62/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.1706e-04\n",
      "Epoch 63/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 2.3842e-05\n",
      "Epoch 64/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 7.8457e-04\n",
      "Epoch 65/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0030\n",
      "Epoch 66/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 3.5763e-07\n",
      "Epoch 67/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 1.0418e-04\n",
      "Epoch 68/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.3947e-05\n",
      "Epoch 69/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 70/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0600\n",
      "Epoch 71/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0424\n",
      "Epoch 72/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 73/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.6212e-05\n",
      "Epoch 74/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 75/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 5.8411e-05\n",
      "Epoch 76/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.8835e-05\n",
      "Epoch 77/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.4066e-04\n",
      "Epoch 78/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 6.1989e-06\n",
      "Epoch 79/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 8.7022e-06\n",
      "Epoch 80/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0830\n",
      "Epoch 81/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 1.3590e-05\n",
      "Epoch 82/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 2.0266e-06\n",
      "Epoch 83/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0061\n",
      "Epoch 84/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 7.5337e-05\n",
      "Epoch 85/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0038\n",
      "Epoch 86/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.4638e-04\n",
      "Epoch 87/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.6332e-05\n",
      "Epoch 88/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 3.8147e-06\n",
      "Epoch 89/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 9.4175e-06\n",
      "Epoch 90/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.4543e-05\n",
      "Epoch 91/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 7.6294e-06\n",
      "Epoch 92/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 8.2254e-06\n",
      "Epoch 93/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 1.8716e-05\n",
      "Epoch 94/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 8.3446e-07\n",
      "Epoch 95/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 96/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 6.6278e-05\n",
      "Epoch 97/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0143\n",
      "Epoch 98/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 1.5424e-04\n",
      "Epoch 99/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0050\n",
      "Epoch 100/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 101/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 4.8876e-06\n",
      "Epoch 102/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0028\n",
      "Epoch 103/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.2636e-05\n",
      "Epoch 104/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0094\n",
      "Epoch 105/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 4.4345e-05\n",
      "Epoch 106/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.8239e-05\n",
      "Epoch 107/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.1086e-05\n",
      "Epoch 108/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 8.3446e-07\n",
      "Epoch 109/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 4.3836e-04\n",
      "Epoch 110/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 5.9605e-07\n",
      "Epoch 111/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 2.7534e-04\n",
      "Epoch 112/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 113/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 1.1038e-04\n",
      "Epoch 114/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 3.5763e-07\n",
      "Epoch 115/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0067\n",
      "Epoch 116/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 4.2676e-05\n",
      "Epoch 117/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 5.9605e-07\n",
      "Epoch 118/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 119/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 4.7684e-07\n",
      "Epoch 120/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 3.1471e-05\n",
      "Epoch 121/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0123\n",
      "Epoch 122/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 2.3842e-07\n",
      "Epoch 123/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.9073e-05\n",
      "Epoch 124/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.0252e-05\n",
      "Epoch 125/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.5497e-06\n",
      "Epoch 126/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 127/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0194\n",
      "Epoch 128/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.1921e-07\n",
      "Epoch 129/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.5985e-04\n",
      "Epoch 130/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 6.8772e-04\n",
      "Epoch 131/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.7404e-05\n",
      "Epoch 132/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 7.1526e-07\n",
      "Epoch 133/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 6.4373e-06\n",
      "Epoch 134/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.4185e-04\n",
      "Epoch 135/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 2.3842e-07\n",
      "Epoch 136/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 4.7684e-07\n",
      "Epoch 137/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 138/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 2.4080e-05\n",
      "Epoch 139/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 4.1961e-05\n",
      "Epoch 140/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 6.8424e-05\n",
      "Epoch 141/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 2.1458e-06\n",
      "Epoch 142/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 2.3842e-07\n",
      "Epoch 143/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 2.2530e-05\n",
      "Epoch 144/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 145/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 5.0866e-04\n",
      "Epoch 146/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.1921e-07\n",
      "Epoch 147/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 148/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 2.0266e-06\n",
      "Epoch 149/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.7296e-04\n",
      "Epoch 150/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.5736e-05\n",
      "Epoch 151/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 3.8557e-04\n",
      "Epoch 152/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 2.0967e-04\n",
      "Epoch 153/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 9.5367e-07\n",
      "Epoch 154/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 4.3859e-04\n",
      "Epoch 155/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 4.6052e-04\n",
      "Epoch 156/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 4.1246e-05\n",
      "Epoch 157/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.1921e-07\n",
      "Epoch 158/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 2.2530e-05\n",
      "Epoch 159/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.6332e-05\n",
      "Epoch 160/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0157\n",
      "Epoch 161/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.8133e-05\n",
      "Epoch 162/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 3.5763e-07\n",
      "Epoch 163/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 164/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.5497e-06\n",
      "Epoch 165/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 7.2882e-04\n",
      "Epoch 166/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 9.1791e-06\n",
      "Epoch 167/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 168/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.8954e-05\n",
      "Epoch 169/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 1.9382e-04\n",
      "Epoch 170/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 3.0191e-04\n",
      "Epoch 171/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.6509e-04\n",
      "Epoch 172/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 7.2717e-06\n",
      "Epoch 173/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 1.0000 - loss: 1.4543e-05\n",
      "Epoch 174/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 175/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 2.3842e-07\n",
      "Epoch 176/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 8.5827e-05\n",
      "Epoch 177/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 3.8147e-06\n",
      "Epoch 178/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 8.3446e-07\n",
      "Epoch 179/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 2.3842e-06\n",
      "Epoch 180/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 7.0106e-04\n",
      "Epoch 181/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.1921e-07\n",
      "Epoch 182/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 5.9605e-07\n",
      "Epoch 183/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.1563e-05\n",
      "Epoch 184/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 6.4373e-06\n",
      "Epoch 185/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 1.5974e-05\n",
      "Epoch 186/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 2.5749e-05\n",
      "Epoch 187/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 2.2650e-06\n",
      "Epoch 188/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.6689e-06\n",
      "Epoch 189/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.1921e-07\n",
      "Epoch 190/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 4.9448e-04\n",
      "Epoch 191/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 192/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 193/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 6.9141e-06\n",
      "Epoch 194/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 195/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 8.3446e-07\n",
      "Epoch 196/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 197/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 198/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 2.1110e-04\n",
      "Epoch 199/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 200/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 1.7524e-05\n",
      "Epoch 201/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 2.0266e-06\n",
      "Epoch 202/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 2.3842e-07\n",
      "Epoch 203/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 3.9339e-06\n",
      "Epoch 204/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 1.0000 - loss: 7.4573e-04\n",
      "Epoch 205/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 1.0729e-06\n",
      "Epoch 206/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 1.4424e-05\n",
      "Epoch 207/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 8.9761e-05\n",
      "Epoch 208/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 209/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 210/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 2.8133e-05\n",
      "Epoch 211/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 212/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 1.1921e-07\n",
      "Epoch 213/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 5.3881e-05\n",
      "Epoch 214/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.0395e-04\n",
      "Epoch 215/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.9947e-05\n",
      "Epoch 216/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 1.3935e-04\n",
      "Epoch 217/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 2.9536e-04\n",
      "Epoch 218/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.3196e-04\n",
      "Epoch 219/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 2.7537e-05\n",
      "Epoch 220/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 2.3842e-07\n",
      "Epoch 221/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 5.9605e-07\n",
      "Epoch 222/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 7.1526e-07\n",
      "Epoch 223/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0043\n",
      "Epoch 224/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.2199e-05\n",
      "Epoch 225/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 5.8412e-06\n",
      "Epoch 226/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.1921e-07\n",
      "Epoch 227/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 9.0953e-05\n",
      "Epoch 228/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 4.8040e-05\n",
      "Epoch 229/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.5497e-06\n",
      "Epoch 230/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 1.7881e-06\n",
      "Epoch 231/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 232/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 233/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 234/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 235/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 2.5972e-04\n",
      "Epoch 236/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 6.1629e-05\n",
      "Epoch 237/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0028\n",
      "Epoch 238/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 5.6263e-04\n",
      "Epoch 239/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 1.0000 - loss: 3.8676e-04\n",
      "Epoch 240/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 2.3184e-04\n",
      "Epoch 241/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 5.3644e-06\n",
      "Epoch 242/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.5020e-05\n",
      "Epoch 243/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.0371e-05\n",
      "Epoch 244/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 2.9802e-06\n",
      "Epoch 245/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 4.2914e-05\n",
      "Epoch 246/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 3.0994e-06\n",
      "Epoch 247/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 1.2051e-04\n",
      "Epoch 248/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 2.8491e-05\n",
      "Epoch 249/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 250/250\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 1.1921e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created.\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "print(\"Building model...\")\n",
    "history = model.fit(trainX, trainY, epochs=250, batch_size=5, verbose=1)\n",
    "model.save('./model/chatbotModel.h5', history) \n",
    "print('Model created.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
